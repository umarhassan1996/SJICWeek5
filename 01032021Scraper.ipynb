{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01032021Scraper.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEWyRaW7LClpR3kASJW/sq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umarhassan1996/SJICWeek5/blob/main/01032021Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJrGZmzC6cg"
      },
      "source": [
        "##An example scraper using a list\r\n",
        "\r\n",
        "The code below can be copied and adapted to create your own scraper.\r\n",
        "\r\n",
        "The first part installs all the libraries. I've kept this separate to the other parts so that you don't have to install them every time you want to run the scraper itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIR1r_tHU_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87907acd-a672-431a-af67-10da5aff1aac"
      },
      "source": [
        "#install the libraries \r\n",
        "#scraperwiki is a library for scraping webpages\r\n",
        "!pip install scraperwiki\r\n",
        "import scraperwiki\r\n",
        "#lxml.html is used to convert it into xml (more structured)\r\n",
        "import lxml.html\r\n",
        "#cssselect is used to drill down into that and find data in tags\r\n",
        "!pip install cssselect\r\n",
        "import cssselect\r\n",
        "#the pandas library which is used to work with data - we call it 'pd' here so we have to type less!\r\n",
        "import pandas as pd"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scraperwiki in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (1.5.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (1.15.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (1.3.23)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->scraperwiki) (2.8.1)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->scraperwiki) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->scraperwiki) (1.1.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->scraperwiki) (1.1.1)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBF7xNHmDKJj"
      },
      "source": [
        "##Using a list\r\n",
        "\r\n",
        "Below we write some code to create a list of counties that can be used to generate URLs on a karting site.\r\n",
        "\r\n",
        "We also store the 'base URL' that we will add to each item in the list to create a full URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5armCoDO5y"
      },
      "source": [
        "#create a list of dates that we will need to generate URLs\r\n",
        "journojobtypes = [\"online\",\"nationals\",\"b2b\"]\r\n",
        "#store the base URL we will add those to\r\n",
        "baseurl = \"https://www.cisionjobs.co.uk/jobs/journalist/\""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEKDES0pEvTn"
      },
      "source": [
        "## Using a loop\r\n",
        "\r\n",
        "Next we loop through each item in the list and add it to that base url using the + operator.\r\n",
        "\r\n",
        "We add a print function inside the loop to check that it works each time - and copy those links into a browser to check that they are the right links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAOQBkMqE1G1",
        "outputId": "dce6b1aa-6ff6-464d-f6d8-27627f27b125"
      },
      "source": [
        "#start looping through our list\r\n",
        "for i in journojobtypes:\r\n",
        "  fullurl = baseurl+i\r\n",
        "  print(fullurl)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.cisionjobs.co.uk/jobs/journalist/online\n",
            "https://www.cisionjobs.co.uk/jobs/journalist/nationals\n",
            "https://www.cisionjobs.co.uk/jobs/journalist/b2b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tISFtygvFAqr"
      },
      "source": [
        "## Scraping each URL as we loop\r\n",
        "\r\n",
        "Now that we know the loop works in generating the right URLs, we can extend the code inside the loop so that it scrapes each URL.\r\n",
        "\r\n",
        "At this point we are using some of the libraries we imported at the start. scraperwiki.scrape(), for example, is the scrape() function from the scraperwiki library.\r\n",
        "\r\n",
        "Let's look at the code first, and then explain it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5MI9ghRFEwe",
        "outputId": "26ec21be-d158-49f6-8fd9-c877bc64ea35"
      },
      "source": [
        "for i in journojobtypes:\r\n",
        "  fullurl = baseurl+i\r\n",
        "  print(fullurl)\r\n",
        "  #Scrape the html at that url\r\n",
        "  html = scraperwiki.scrape(fullurl)\r\n",
        "  # turn our HTML into an lxml object\r\n",
        "  root = lxml.html.fromstring(html) \r\n",
        "  #The names are all in <td> and <a>\r\n",
        "  #This targets the contents of those html tags\r\n",
        "jobs = root.cssselect('span')\r\n",
        "  #the results are always a list so we have to loop through it using a 'for' loop\r\n",
        "for i in jobs:\r\n",
        "    #each item in the list is called i as it loops\r\n",
        "    print(i)\r\n",
        "    #on its own it looks odd, but we can attach .text_content() to translate it into text\r\n",
        "    job = i.text_content()\r\n",
        "    print(job)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.cisionjobs.co.uk/jobs/journalist/online\n",
            "https://www.cisionjobs.co.uk/jobs/journalist/nationals\n",
            "https://www.cisionjobs.co.uk/jobs/journalist/b2b\n",
            "<Element span at 0x7fde4d9fa110>\n",
            "Skip to main menu\n",
            "<Element span at 0x7fde4d9fa170>\n",
            "Skip to user menu\n",
            "<Element span at 0x7fde519b4e30>\n",
            "Jobseekers\n",
            "<Element span at 0x7fde51a08dd0>\n",
            "or\n",
            "<Element span at 0x7fde519f2530>\n",
            "\n",
            "\t\t\t\n",
            "\t\t\t\tUS\n",
            "\t\t\t\n",
            "\t\t\t\n",
            "\t\t\t\tEU\n",
            "\t\t\t\n",
            "\t\t\n",
            "<Element span at 0x7fde4d987050>\n",
            "\n",
            "<Element span at 0x7fde4f86fdd0>\n",
            "Email\n",
            "<Element span at 0x7fde4da6ccb0>\n",
            "\n",
            "<Element span at 0x7fde4da6c5f0>\n",
            "Remove selection\n",
            "<Element span at 0x7fde4da6ca70>\n",
            "\n",
            "<Element span at 0x7fde4da6c0b0>\n",
            "Remove selection\n",
            "<Element span at 0x7fde4da6cad0>\n",
            "\n",
            "<Element span at 0x7fde4da6c290>\n",
            "\n",
            "<Element span at 0x7fde4da6c170>\n",
            "\n",
            "<Element span at 0x7fde4da6c590>\n",
            "\n",
            "<Element span at 0x7fde4da6c2f0>\n",
            "\n",
            "<Element span at 0x7fde4da6c710>\n",
            "\n",
            "<Element span at 0x7fde4da6cf50>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4da6c770>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4da6c890>\n",
            "Save Trading Risk, Reporter\n",
            "<Element span at 0x7fde4daa00b0>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4daa0110>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4daa0170>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4daa01d0>\n",
            "Save MCA, Editor \n",
            "<Element span at 0x7fde4daa0fb0>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4daa0050>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4d990a10>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4d990830>\n",
            "Save MCA, Editor \n",
            "<Element span at 0x7fde4d990b30>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4d9909b0>\n",
            "Global Restructuring Review, News Reporter\n",
            "<Element span at 0x7fde4d990d70>\n",
            "Global Restructuring Review, News Reporter\n",
            "<Element span at 0x7fde4daeb590>\n",
            "Save Global Restructuring Review, News Reporter\n",
            "<Element span at 0x7fde4daeb830>\n",
            "Global Restructuring Review, News Reporter\n",
            "<Element span at 0x7fde4daeb890>\n",
            "Global Banking Regulation Review, Trainee News Reporter\n",
            "<Element span at 0x7fde4daeb8f0>\n",
            "Global Banking Regulation Review, Trainee News Reporter\n",
            "<Element span at 0x7fde4daeb950>\n",
            "Save Global Banking Regulation Review, Trainee News Reporter\n",
            "<Element span at 0x7fde4daeb2f0>\n",
            "Global Banking Regulation Review, Trainee News Reporter\n",
            "<Element span at 0x7fde4daeb350>\n",
            "B2B Editor \n",
            "<Element span at 0x7fde4daebe90>\n",
            "B2B Editor \n",
            "<Element span at 0x7fde4daeb710>\n",
            "Save B2B Editor \n",
            "<Element span at 0x7fde4daebef0>\n",
            "B2B Editor \n",
            "<Element span at 0x7fde4daeb470>\n",
            "Employee Benefits, Reporter \n",
            "<Element span at 0x7fde4daeb3b0>\n",
            "Employee Benefits, Reporter \n",
            "<Element span at 0x7fde4daeb4d0>\n",
            "Save Employee Benefits, Reporter \n",
            "<Element span at 0x7fde4daeb530>\n",
            "Employee Benefits, Reporter \n",
            "<Element span at 0x7fde4f4377d0>\n",
            "Gaming Intelligence, Editorial Internship\n",
            "<Element span at 0x7fde4f437e30>\n",
            "Gaming Intelligence, Editorial Internship\n",
            "<Element span at 0x7fde4da6ea10>\n",
            "Save Gaming Intelligence, Editorial Internship\n",
            "<Element span at 0x7fde4da6ec50>\n",
            "Gaming Intelligence, Editorial Internship\n",
            "<Element span at 0x7fde4da6eb90>\n",
            "Gaming Intelligence, Multimedia Editor\n",
            "<Element span at 0x7fde4da6e950>\n",
            "Gaming Intelligence, Multimedia Editor\n",
            "<Element span at 0x7fde4da6ed70>\n",
            "Save Gaming Intelligence, Multimedia Editor\n",
            "<Element span at 0x7fde4da6edd0>\n",
            "Gaming Intelligence, Multimedia Editor\n",
            "<Element span at 0x7fde4da6e7d0>\n",
            "Gaming Intelligence, B2B Reporter\n",
            "<Element span at 0x7fde4da6e650>\n",
            "Gaming Intelligence, B2B Reporter\n",
            "<Element span at 0x7fde4da6e8f0>\n",
            "Save Gaming Intelligence, B2B Reporter\n",
            "<Element span at 0x7fde4da6e770>\n",
            "Gaming Intelligence, B2B Reporter\n",
            "<Element span at 0x7fde4da6ebf0>\n",
            "The Media Eye, Researcher & Content Writer - Celebrity Team\n",
            "<Element span at 0x7fde4da6e350>\n",
            "The Media Eye, Researcher & Content Writer - Celebrity Team\n",
            "<Element span at 0x7fde4da6e410>\n",
            "Save The Media Eye, Researcher & Content Writer - Celebrity Team\n",
            "<Element span at 0x7fde4da6e530>\n",
            "The Media Eye, Researcher & Content Writer - Celebrity Team\n",
            "<Element span at 0x7fde4da6e590>\n",
            "Distillery, Writer\n",
            "<Element span at 0x7fde4da6e5f0>\n",
            "Distillery, Writer\n",
            "<Element span at 0x7fde4da6e9b0>\n",
            "Save Distillery, Writer\n",
            "<Element span at 0x7fde4da6e470>\n",
            "Distillery, Writer\n",
            "<Element span at 0x7fde4da6e4d0>\n",
            "Ground Engineering, Senior Reporter \n",
            "<Element span at 0x7fde4da6e2f0>\n",
            "Ground Engineering, Senior Reporter \n",
            "<Element span at 0x7fde4da6e710>\n",
            "3 days left\n",
            "<Element span at 0x7fde4da6e6b0>\n",
            "Save Ground Engineering, Senior Reporter \n",
            "<Element span at 0x7fde4da6e1d0>\n",
            "Ground Engineering, Senior Reporter \n",
            "<Element span at 0x7fde4da6e230>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4da6e290>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4da6e3b0>\n",
            "Save Trading Risk, Reporter\n",
            "<Element span at 0x7fde4dae30b0>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4dae3230>\n",
            "British Baker, Reporter\n",
            "<Element span at 0x7fde4dae3950>\n",
            "British Baker, Reporter\n",
            "<Element span at 0x7fde4dae3050>\n",
            "Save British Baker, Reporter\n",
            "<Element span at 0x7fde4dae3110>\n",
            "British Baker, Reporter\n",
            "<Element span at 0x7fde4dae3170>\n",
            "Construction News, Reporter\n",
            "<Element span at 0x7fde4dae31d0>\n",
            "Construction News, Reporter\n",
            "<Element span at 0x7fde4dae3290>\n",
            "Save Construction News, Reporter\n",
            "<Element span at 0x7fde4dae32f0>\n",
            "Construction News, Reporter\n",
            "<Element span at 0x7fde4dae3350>\n",
            "Construction News, News Editor\n",
            "<Element span at 0x7fde4dae33b0>\n",
            "Construction News, News Editor\n",
            "<Element span at 0x7fde4dae3410>\n",
            "Save Construction News, News Editor\n",
            "<Element span at 0x7fde4dae3470>\n",
            "Construction News, News Editor\n",
            "<Element span at 0x7fde4dae34d0>\n",
            "Reporter\n",
            "<Element span at 0x7fde4dae3530>\n",
            "Reporter\n",
            "<Element span at 0x7fde4dae3590>\n",
            "3 days left\n",
            "<Element span at 0x7fde4dae35f0>\n",
            "Save Reporter\n",
            "<Element span at 0x7fde4dae3650>\n",
            "Reporter\n",
            "<Element span at 0x7fde4dae36b0>\n",
            "Incisive Works, Editor - TechnologyContent\n",
            "<Element span at 0x7fde4dae3710>\n",
            "Incisive Works, Editor - TechnologyContent\n",
            "<Element span at 0x7fde4dae3ad0>\n",
            "1 day left\n",
            "<Element span at 0x7fde4dae3a70>\n",
            "Save Incisive Works, Editor - TechnologyContent\n",
            "<Element span at 0x7fde4dae3a10>\n",
            "Incisive Works, Editor - TechnologyContent\n",
            "<Element span at 0x7fde4dae38f0>\n",
            "Incisive Works, Editor - Financial Services Content\n",
            "<Element span at 0x7fde4dae3890>\n",
            "Incisive Works, Editor - Financial Services Content\n",
            "<Element span at 0x7fde4dae3b90>\n",
            "1 day left\n",
            "<Element span at 0x7fde4dae3bf0>\n",
            "Save Incisive Works, Editor - Financial Services Content\n",
            "<Element span at 0x7fde4dae3c50>\n",
            "Incisive Works, Editor - Financial Services Content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TctdwjOjGIPj"
      },
      "source": [
        "## The functions we are using\r\n",
        "\r\n",
        "## Let's break some of this down.\r\n",
        "\r\n",
        "So scraperwiki.scrape() is the scrape() function from the scraperwiki library. The ingredient we give to that function is the URL we stored in the fullurl variable.\r\n",
        "\r\n",
        "The scrape() function basically fetches the whole webpage at a given address (the ingredient it's given).\r\n",
        "\r\n",
        "The results of running that function are stored in a new variable called html.\r\n",
        "\r\n",
        "This isn't in a form we can easily work with, yet, so we need another function to convert it to something we can drill down into.\r\n",
        "\r\n",
        "That function is the fromstring() function from the lxml.html library. The ingredient we give to that function is the html variable we just created.\r\n",
        "\r\n",
        "The results are stored in another new variable, root.\r\n",
        "\r\n",
        "This variable is a particular type of object (an \"lxml object\" if you need to know) that can be drilled down into using the cssselect function. That function will grab elements that match the CSS selectors that you give it as an ingredient.\r\n",
        "\r\n",
        "In this case we specify 'h2', which means \"any h2 tag\" - so it will grab the contents of any h2 tags in the page.\r\n",
        "\r\n",
        "Don't worry about memorising any of the code above: this is code that you can re-use time and time again. The only bit you will need to change is the selector, in order to specify the particular HTML you're after.\r\n",
        "\r\n",
        "To work out the selector you need, you'll often need to Google around, learning as you go, but selectors are pretty easy to get the hang of, and I'll talk about it more below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwctVtLMGzI7"
      },
      "source": [
        "## Using CSS selectors\r\n",
        "\r\n",
        "CSS selectors are used to target different elements in a HTML page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K9FHkIEHBRR"
      },
      "source": [
        "## Saving the information we've grabbed.\r\n",
        "\r\n",
        "Now we've grabbed some information we can extend the code further to save it.\r\n",
        "\r\n",
        "At this point we need to use functions from another library: pandas. This is a library for data storage and analysis. When we imported pandas we called it pd for short. This is quite common. Any reference to pd in the code, then, means pandas\r\n",
        "\r\n",
        "First, we use the function DataFrame() which creates a pandas dataframe. As ingredients it needs to know the names of any columns.\r\n",
        "\r\n",
        "You will see below that we add a line before the loop which uses that to create an empty dataframe to store the data in.\r\n",
        "\r\n",
        "Then, inside the loop, the data we extract is added to the dataframe.\r\n",
        "\r\n",
        "Here's the code first - then I'll explain the new bits after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzSbhRYXHAsW",
        "outputId": "b0f26e76-a71c-44ba-f76d-2946577b5736"
      },
      "source": [
        "#Create a dataframe to store the data we are about to scrape\r\n",
        "#It has one column called 'title'\r\n",
        "#We call this dataframe 'df'\r\n",
        "df = pd.DataFrame(columns=[\"job-role\"])\r\n",
        "\r\n",
        "for i in journojobtypes:\r\n",
        "  fullurl = baseurl+i\r\n",
        "  print(fullurl)\r\n",
        "  #Scrape the html at that url\r\n",
        "  html = scraperwiki.scrape(fullurl)\r\n",
        "  # turn our HTML into an lxml object\r\n",
        "  root = lxml.html.fromstring(html) \r\n",
        "  #The names are all in <td> and <a>\r\n",
        "  #This targets the contents of those html tags\r\n",
        "jobs = root.cssselect('h3 span')\r\n",
        "  #the results are always a list so we have to loop through it using a 'for' loop\r\n",
        "for i in jobs:\r\n",
        "    #each item in the list is called i as it loops\r\n",
        "    print(i)\r\n",
        "    #on its own it looks odd, but we can attach .text_content() to translate it into text\r\n",
        "    job = i.text_content()\r\n",
        "    print(job)\r\n",
        "    #Now we need to store it in that variable called 'df' \r\n",
        "    df = df.append({\r\n",
        "      \"job-role\" : job\r\n",
        "      }, ignore_index=True)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.cisionjobs.co.uk/jobs/journalist/online\n",
            "https://www.cisionjobs.co.uk/jobs/journalist/nationals\n",
            "https://www.cisionjobs.co.uk/jobs/journalist/b2b\n",
            "<Element span at 0x7fde4f3ca1d0>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4f3ca830>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4f86d0b0>\n",
            "MCA, Editor \n",
            "<Element span at 0x7fde4f86dd70>\n",
            "Global Restructuring Review, News Reporter\n",
            "<Element span at 0x7fde4d987050>\n",
            "Global Banking Regulation Review, Trainee News Reporter\n",
            "<Element span at 0x7fde51a2ead0>\n",
            "B2B Editor \n",
            "<Element span at 0x7fde4da0d1d0>\n",
            "Employee Benefits, Reporter \n",
            "<Element span at 0x7fde4f8700b0>\n",
            "Gaming Intelligence, Editorial Internship\n",
            "<Element span at 0x7fde4f3a9bf0>\n",
            "Gaming Intelligence, Multimedia Editor\n",
            "<Element span at 0x7fde4d990830>\n",
            "Gaming Intelligence, B2B Reporter\n",
            "<Element span at 0x7fde4d9908f0>\n",
            "The Media Eye, Researcher & Content Writer - Celebrity Team\n",
            "<Element span at 0x7fde4f3e54d0>\n",
            "Distillery, Writer\n",
            "<Element span at 0x7fde4da6c530>\n",
            "Ground Engineering, Senior Reporter \n",
            "<Element span at 0x7fde4da6cd70>\n",
            "Trading Risk, Reporter\n",
            "<Element span at 0x7fde4da6c950>\n",
            "British Baker, Reporter\n",
            "<Element span at 0x7fde4da6c230>\n",
            "Construction News, Reporter\n",
            "<Element span at 0x7fde4da6c2f0>\n",
            "Construction News, News Editor\n",
            "<Element span at 0x7fde4da6c4d0>\n",
            "Reporter\n",
            "<Element span at 0x7fde4da6ccb0>\n",
            "Incisive Works, Editor - TechnologyContent\n",
            "<Element span at 0x7fde4da6c470>\n",
            "Incisive Works, Editor - Financial Services Content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3VGnEQwKRXI"
      },
      "source": [
        "## The new code\r\n",
        "\r\n",
        "The first line of new code is this:\r\n",
        "\r\n",
        "df = pd.DataFrame(columns=[\"title\"])\r\n",
        "\r\n",
        "We are creating a new variable here, called df, and assigning to it the results of using a function: pd.DataFrame() (the pandas function DataFrame).\r\n",
        "\r\n",
        "That takes an ingredient which specifies the columns as being a list (note the square brackets) of one string: \"title\".\r\n",
        "\r\n",
        "The second line of new code is this:\r\n",
        "\r\n",
        "df = df.append({\r\n",
        "      \"title\" : title\r\n",
        "      }, ignore_index=True)\r\n",
        "This takes the df variable and updates it.\r\n",
        "\r\n",
        "On the right of the equals sign is df.append() - this means it is using a function called append to append (add) new data to the df variable it's attached to.\r\n",
        "\r\n",
        "The append function can include various ingredients: firstly the data that you want to append to the dataframe; but also settings, such as whether you want something called ignore_index to be True or False. Setting this to True just avoids problems when your data isn't unique.\r\n",
        "\r\n",
        "What about the data that you are appending? Well, this has to be in the form of a dictionary. A dictionary is like a list, but with two key differences: firstly that it uses curly brackets instead of square ones: {}, and secondly it's a list of pairs: a 'key', and a 'value', separated by a colon.\r\n",
        "\r\n",
        "Here's the dictionary in our code:\r\n",
        "\r\n",
        "{\"title\" : title}\r\n",
        "\r\n",
        "The first part, \"title\" is the key. This matches the column heading in the empty data frame. Note that it's a string: a label, basically.\r\n",
        "\r\n",
        "The second part, title, is the value. This isn't in quotes so it's not a string - it's a variable. A few lines earlier we created this variable with title = i.text_content()\r\n",
        "\r\n",
        "So having extracted that information and stored it in title, the line of code is storing it in a dataframe with the label (key) \"title\":\r\n",
        "\r\n",
        "df = df.append({\r\n",
        "      \"title\" : title\r\n",
        "      }, ignore_index=True)\r\n",
        "\r\n",
        "We can print the dataframe to see what's in there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYTHfEWIKV92",
        "outputId": "96c111b8-ccfa-45c2-c419-66a4c71e5aad"
      },
      "source": [
        "#Once the loop has finished we can take a look at the data\r\n",
        "print(df)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             job-role\n",
            "0                              Trading Risk, Reporter\n",
            "1                                        MCA, Editor \n",
            "2                                        MCA, Editor \n",
            "3          Global Restructuring Review, News Reporter\n",
            "4   Global Banking Regulation Review, Trainee News...\n",
            "5                                         B2B Editor \n",
            "6                        Employee Benefits, Reporter \n",
            "7           Gaming Intelligence, Editorial Internship\n",
            "8              Gaming Intelligence, Multimedia Editor\n",
            "9                   Gaming Intelligence, B2B Reporter\n",
            "10  The Media Eye, Researcher & Content Writer - C...\n",
            "11                                 Distillery, Writer\n",
            "12               Ground Engineering, Senior Reporter \n",
            "13                             Trading Risk, Reporter\n",
            "14                            British Baker, Reporter\n",
            "15                        Construction News, Reporter\n",
            "16                     Construction News, News Editor\n",
            "17                                           Reporter\n",
            "18         Incisive Works, Editor - TechnologyContent\n",
            "19  Incisive Works, Editor - Financial Services Co...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q2wiWibPhdx"
      },
      "source": [
        "##Exporting the data\r\n",
        "\r\n",
        "The pandas library has another function for exporting data: to_csv().\r\n",
        "\r\n",
        "It needs to be attached to the name of the dataframe variable with a period, then, in the brackets, you specify the name of the file you want to export it as. Make sure this ends in '.csv' so it can be used in a spreadsheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3WnfTIkPqTr"
      },
      "source": [
        "#And we can export it\r\n",
        "df.to_csv(\"journojobs.csv\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tLWrfaRQNTC"
      },
      "source": [
        "##That's it. I have managed to use a list to scrape through a list of URLs. Unbelievable."
      ]
    }
  ]
}